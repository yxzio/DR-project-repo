{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cce57ed5-2cf4-4af0-bafb-c46c4233f599",
   "metadata": {},
   "source": [
    "# CSE 398 Deep Learning Final Project\n",
    "## Detecting Diabetic Retinopathy in Enhanced Retinal Fundus Images\n",
    "### DeepNet Architecture Tests with Low-Quality and Enhanced Low-Quality Retinal Fundus Images\n",
    "\n",
    "James Hoffmeister"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca6e92a-65e2-42ef-bb1d-44d1affeeb7a",
   "metadata": {},
   "source": [
    "### tdqm\n",
    "\n",
    "We install TDQM to have progress bars while training. This helps understand how fast the models may train and validate data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80657d10-6494-4fa8-ae46-e2c94a2b4650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tqdm in /opt/tljh/user/lib/python3.9/site-packages (4.62.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251b46b0-638f-4a50-9960-fa30ef54949d",
   "metadata": {},
   "source": [
    "## Imports and Transforms\n",
    "\n",
    "We define transforms thattranslate the original 512x512 image format into 224x224 imagenet standard for the DenseNet model. We also add some preprocessing to reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7a3199a-2cbb-445a-86a5-9f05edea6237",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /opt/tljh/user/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefIlEENS2_8optionalINS2_10ScalarTypeEEENS5_INS2_6LayoutEEENS5_INS2_6DeviceEEENS5_IbEENS5_INS2_12MemoryFormatEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA\n",
      "Transformers defined\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from torch.utils.data import Subset\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "import torch\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "if torch.cuda.is_available:\n",
    "  device = torch.device(\"cuda\")\n",
    "  print('Using CUDA')\n",
    "else:\n",
    "  device = torch.device(\"cpu\")\n",
    "  print('Could not use CUDA')\n",
    "\n",
    "# image transform to DenseNet input architecture\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(512, scale=(0.8, 1.0)),  # Random crop but not too crazy\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=15),  # Mild rotation\n",
    "    transforms.ColorJitter(\n",
    "        brightness=0.2,\n",
    "        contrast=0.2,\n",
    "        saturation=0.2,\n",
    "        hue=0.02\n",
    "    ),\n",
    "    transforms.RandomApply([\n",
    "        transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5))\n",
    "    ], p=0.3),  # Some random blurring\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],  # Standard ImageNet normalization\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    ),\n",
    "])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n",
    "])\n",
    "\n",
    "print(\"Transformers defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1da3f4-ecbb-480e-b3f5-0546e936dd83",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "We load the data from the directories using a filtered image folder object that detects only images that may be used for training and testing. We stratify the data before splitting it into training and validation subsets randomly. \n",
    "\n",
    "I am more used to Numpy than Torch so sometimes for tensor operations I default to Numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45df5135-219b-49a2-9765-28f8d6af7332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsets defined\n"
     ]
    }
   ],
   "source": [
    "# this was only necessary because magic had some invisible file\n",
    "# in my folder that I had to tell the code to ignore ¯\\_(ツ)_/¯\n",
    "class FilteredImageFolder(ImageFolder):\n",
    "    def find_classes(self, directory):\n",
    "        # ignore the .ipynb file hidden in my train folder\n",
    "        classes = [d for d in os.listdir(directory) if os.path.isdir(os.path.join(directory, d)) and not d.startswith('.')]\n",
    "        classes.sort()\n",
    "        class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n",
    "        return classes, class_to_idx\n",
    "\n",
    "# directory with data\n",
    "data_dir = \"/home/jupyter-jah823/train\"\n",
    "\n",
    "# using imagefolder to extract data\n",
    "dataset = FilteredImageFolder(root=data_dir, transform=None)\n",
    "\n",
    "# stratified split\n",
    "targets = dataset.targets\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_id, val_id = next(splitter.split(np.zeros(len(targets)), targets))\n",
    "\n",
    "# subsets\n",
    "train_dataset = Subset(dataset, train_id)\n",
    "val_dataset = Subset(dataset, val_id)\n",
    "\n",
    "# add transformations\n",
    "train_dataset.dataset.transform = transform_train\n",
    "val_dataset.dataset.transform = transform_val\n",
    "\n",
    "print(\"Subsets defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c38aab2-d00b-49d2-85cd-90edb4ca104f",
   "metadata": {},
   "source": [
    "## Weighted Sampling\n",
    "\n",
    "We weight the sampler because the class distribution is heavily skewed to the class 0. We use power-log scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ad838d2-05ef-45ef-aa30-fd670be4314e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:39<00:00,  3.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1: 3489 samples (34.77%)\n",
      "Class 2: 3339 samples (33.28%)\n",
      "Class 3: 2084 samples (20.77%)\n",
      "Class 0: 559 samples (5.57%)\n",
      "Class 4: 563 samples (5.61%)\n",
      "Data loaders prepared!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# weighted sampler to deal with class imbalance\n",
    "\n",
    "train_targets = torch.tensor([dataset.targets[i] for i in train_dataset.indices], device=device)\n",
    "num_classes = train_targets.max().item() + 1\n",
    "class_counts = torch.bincount(train_targets, minlength=num_classes)\n",
    "class_weights_per_class = 1. / (class_counts.float() ** 0.5)\n",
    "class_weights_per_class[0] *= 0.15\n",
    "class_weights_per_class[1] *= 3\n",
    "class_weights_per_class[2] *= 2\n",
    "class_weights_per_class[3] *= 3\n",
    "class_weights_per_class[4] *= 1\n",
    "samples_weight = class_weights_per_class[train_targets]\n",
    "sampler = WeightedRandomSampler(weights=samples_weight, num_samples=len(samples_weight), replacement=True)\n",
    "\n",
    "# load data\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, sampler=sampler, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
    "\n",
    "import collections\n",
    "\n",
    "# Set model to eval mode if necessary\n",
    "sampled_labels = []\n",
    "\n",
    "# Iterate over ONE epoch (you don't need to train, just inspect batches)\n",
    "c = tqdm(train_loader)\n",
    "for inputs, labels in c:\n",
    "    sampled_labels.extend(labels.cpu().numpy())  # collect all sampled labels\n",
    "\n",
    "# Now count occurrences\n",
    "counter = collections.Counter(sampled_labels)\n",
    "\n",
    "# Display percentages\n",
    "total = sum(counter.values())\n",
    "for label, count in counter.items():\n",
    "    print(f\"Class {label}: {count} samples ({100.0 * count / total:.2f}%)\")\n",
    "\n",
    "print(\"Data loaders prepared!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d706e10e-e8f2-4508-a450-879c5183df04",
   "metadata": {},
   "source": [
    "## Model Definition\n",
    "\n",
    "We choose DenseNet121 for this test. This is considerably larger than the other models being trained for this experiment. It is also similar to one of the models used in the original paper that developed the EyeQ dataset. We freeze all but a few of the layers (denseblock4, norm5, classifier). The classifier has dropout to ensure there is no overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15f0eeec-a654-460f-8104-ebe209a24fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "\n",
    "# load densenet121 pretrained model\n",
    "def make_model():\n",
    "    \n",
    "    model = models.densenet121(weights='IMAGENET1K_V1')\n",
    "    num_ftrs = model.classifier.in_features\n",
    "    # added dropout to fix overfitting\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(model.classifier.in_features, 5)  # 0-4\n",
    "    )\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'denseblock3' in name or 'denseblock4' in name or 'norm5' in name or 'classifier' in name:\n",
    "            param.requires_grad = True\n",
    "        else:\n",
    "            param.requires_grad = False\n",
    "\n",
    "    trainable_params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "\n",
    "    model = model.to(device)\n",
    "    \n",
    "    return model, trainable_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df61afc-ce42-4603-b37a-9d427cfadbbf",
   "metadata": {},
   "source": [
    "## Loss, Optimizer, and Scheduler\n",
    "\n",
    "We use focal loss and the Adam optimizer. The Focal Loss attempts to ensure that underrepresented classes are weighted higher for loss, ensuring a more even distribution of accuracies across classes. We use Cosine-Annealing Learning Rate scheduler to smoothly decrease learning rate over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31cccf5d-350c-4749-be20-e235cb0a2bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lossoperational\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "criterions = [nn.CrossEntropyLoss(label_smoothing=0.1)]\n",
    "\n",
    "# using an ADAM optimizer, mentioned in lecture\n",
    "# also regularizing with weight decay since initial tests showed heavy overfitting\n",
    "\n",
    "print(\"Lossoperational\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "729dcd8d-376d-43aa-959c-306ed2591fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_data(x, y, alpha=0.4):\n",
    "    '''Compute the mixup data. Return mixed inputs, pairs of targets, and lambda'''\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d922be3-9eb4-4eb1-9a13-1e748033c74c",
   "metadata": {},
   "source": [
    "## Model Training Loop\n",
    "\n",
    "The training loop iterates through the epochs and prints training set accuracy, validation set accuracy, and validation accuracy stratified by class. This model has difficulties with memorizing the training data due to its size, so it overfits heavily. Additionally, because of the low number of samples in classes 1 and 3, there is a marked decrease in the accuracy of these classes. Potential fixes could include increased image transformations and oversampling of underrepresented classes. Additionally, the decreasing accuracy for the underrepresented classes implies that either the model is failing to learn the features of these classes due to the lack of training examples. This is the highest performing model I could achieve with DenseNet. Note that overall validation set accuracy is heavily dependent on the class 0 accuracy since the validation sets are representative of the data, which contains many more class 0 images than other classes combined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b68294-b8c0-4421-8243-bbbf537985e0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [1/20] - Train Loss: 205.0342, Train Acc: 0.4984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Acc: 0.1375\n",
      "Accuracy for class 0: 0.00%\n",
      "Accuracy for class 1: 77.47%\n",
      "Accuracy for class 2: 38.95%\n",
      "Accuracy for class 3: 56.72%\n",
      "Accuracy for class 4: 50.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [2/20] - Train Loss: 166.2321, Train Acc: 0.6851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Acc: 0.1479\n",
      "Accuracy for class 0: 0.00%\n",
      "Accuracy for class 1: 64.84%\n",
      "Accuracy for class 2: 55.52%\n",
      "Accuracy for class 3: 35.82%\n",
      "Accuracy for class 4: 56.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [3/20] - Train Loss: 149.0445, Train Acc: 0.7595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Acc: 0.1546\n",
      "Accuracy for class 0: 0.11%\n",
      "Accuracy for class 1: 51.65%\n",
      "Accuracy for class 2: 67.68%\n",
      "Accuracy for class 3: 31.34%\n",
      "Accuracy for class 4: 52.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [4/20] - Train Loss: 142.6888, Train Acc: 0.7811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Acc: 0.1570\n",
      "Accuracy for class 0: 1.08%\n",
      "Accuracy for class 1: 49.45%\n",
      "Accuracy for class 2: 64.36%\n",
      "Accuracy for class 3: 32.84%\n",
      "Accuracy for class 4: 58.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [5/20] - Train Loss: 137.1742, Train Acc: 0.7948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Acc: 0.1642\n",
      "Accuracy for class 0: 1.24%\n",
      "Accuracy for class 1: 35.16%\n",
      "Accuracy for class 2: 75.97%\n",
      "Accuracy for class 3: 31.34%\n",
      "Accuracy for class 4: 58.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [6/20] - Train Loss: 136.4299, Train Acc: 0.7954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Acc: 0.1774\n",
      "Accuracy for class 0: 2.60%\n",
      "Accuracy for class 1: 35.16%\n",
      "Accuracy for class 2: 80.11%\n",
      "Accuracy for class 3: 22.39%\n",
      "Accuracy for class 4: 56.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [7/20] - Train Loss: 131.7641, Train Acc: 0.8094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Acc: 0.2164\n",
      "Accuracy for class 0: 8.77%\n",
      "Accuracy for class 1: 39.56%\n",
      "Accuracy for class 2: 72.10%\n",
      "Accuracy for class 3: 32.84%\n",
      "Accuracy for class 4: 52.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [8/20] - Train Loss: 133.9250, Train Acc: 0.8006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Acc: 0.2260\n",
      "Accuracy for class 0: 9.74%\n",
      "Accuracy for class 1: 38.46%\n",
      "Accuracy for class 2: 75.14%\n",
      "Accuracy for class 3: 28.36%\n",
      "Accuracy for class 4: 52.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [9/20] - Train Loss: 130.0829, Train Acc: 0.8134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Acc: 0.2635\n",
      "Accuracy for class 0: 14.94%\n",
      "Accuracy for class 1: 41.21%\n",
      "Accuracy for class 2: 72.65%\n",
      "Accuracy for class 3: 29.85%\n",
      "Accuracy for class 4: 54.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [10/20] - Train Loss: 133.8055, Train Acc: 0.8038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Acc: 0.2491\n",
      "Accuracy for class 0: 13.15%\n",
      "Accuracy for class 1: 35.71%\n",
      "Accuracy for class 2: 75.14%\n",
      "Accuracy for class 3: 28.36%\n",
      "Accuracy for class 4: 52.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [11/20] - Train Loss: 128.2669, Train Acc: 0.8243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Acc: 0.2595\n",
      "Accuracy for class 0: 14.56%\n",
      "Accuracy for class 1: 39.56%\n",
      "Accuracy for class 2: 73.20%\n",
      "Accuracy for class 3: 28.36%\n",
      "Accuracy for class 4: 52.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [12/20] - Train Loss: 129.8935, Train Acc: 0.8122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Acc: 0.2658\n",
      "Accuracy for class 0: 15.42%\n",
      "Accuracy for class 1: 41.21%\n",
      "Accuracy for class 2: 72.10%\n",
      "Accuracy for class 3: 29.85%\n",
      "Accuracy for class 4: 52.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [13/20] - Train Loss: 126.4930, Train Acc: 0.8203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Acc: 0.2539\n",
      "Accuracy for class 0: 13.74%\n",
      "Accuracy for class 1: 33.52%\n",
      "Accuracy for class 2: 76.24%\n",
      "Accuracy for class 3: 29.85%\n",
      "Accuracy for class 4: 52.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20:  56%|█████▌    | 88/157 [00:22<00:14,  4.81it/s, loss=1.1]  "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt # inspied by hw2\n",
    "\n",
    "# training loop\n",
    "def train(model, train_loader, val_loader, optimizer, scheduler, criterion, verbose, epochs):\n",
    "    \n",
    "    # for plotting, thanks hw2\n",
    "    train_losses = []\n",
    "    val_accuracies = []\n",
    "    train_accuracies = []\n",
    "    \n",
    "    # for number of epochs\n",
    "    for epoch in range(epochs):\n",
    "        # switch model to train mode\n",
    "        model.train()\n",
    "        # vars for recording\n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        # loading bar for easy timing\n",
    "        batch_iterator = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=False) if verbose else train_loader\n",
    "\n",
    "        for inputs, targets in batch_iterator:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            inputs, targets_a, targets_b, lam = mixup_data(inputs, targets, alpha=0.4)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += (lam * (predicted == targets_a).sum().item() + (1 - lam) * (predicted == targets_b).sum().item())\n",
    "            total += outputs.size(0)\n",
    "\n",
    "            if verbose:\n",
    "                batch_iterator.set_postfix(loss=loss.item())\n",
    "\n",
    "\n",
    "        # calculate accuracy and print\n",
    "        train_acc = correct / total\n",
    "        train_losses.append(total_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        print(f\"\\nEpoch [{epoch+1}/{epochs}] - Train Loss: {total_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        val_iterator = tqdm(val_loader, desc=f\"Validating {epoch+1}/{epochs}\", leave=False) if verbose else val_loader\n",
    "        \n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        correct_per_class = torch.zeros(5)\n",
    "        total_per_class = torch.zeros(5)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # for each image in validation set\n",
    "            for images, labels in val_iterator:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "                \n",
    "                # classwise accuracy\n",
    "                for label, pred in zip(labels, predicted):\n",
    "                    total_per_class[label] += 1\n",
    "                    if label == pred:\n",
    "                        correct_per_class[label] += 1\n",
    "                \n",
    "                if verbose: val_iterator.set_postfix(loss=loss.item())\n",
    "\n",
    "        val_acc = val_correct / val_total\n",
    "        val_accuracies.append(val_acc)\n",
    "        print(f\"Validation Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        for i in range(5):\n",
    "            if total_per_class[i] > 0:\n",
    "                acc = 100 * correct_per_class[i] / total_per_class[i]\n",
    "                print(f\"Accuracy for class {i}: {acc:.2f}%\")\n",
    "            else:\n",
    "                print(f\"Class {i} has no samples.\")\n",
    "    \n",
    "        # step the scheduler to reduce learning rate\n",
    "        avg_val_loss = val_loss / len(val_iterator)\n",
    "\n",
    "        # step the scheduler\n",
    "        scheduler.step()\n",
    "        \n",
    "    return train_losses, train_accuracies, val_accuracies\n",
    "\n",
    "for c in criterions:\n",
    "    model, trainable_params = make_model()\n",
    "    \n",
    "    optimizer = optim.Adam(trainable_params, lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "    # scheduler to bring down learning rate to reduce overfitting\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "    \n",
    "    train_losses, train_accuracies, val_accuracies = train(model, train_loader, val_loader, optimizer, scheduler, c, verbose=True, epochs=20)\n",
    "    epochs_range = range(1, len(train_losses)+1)\n",
    "\n",
    "    plt.figure(figsize=(12,5))\n",
    "\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(epochs_range, train_losses, label='Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss / Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(epochs_range, train_accuracies, label='Training Accuracy')\n",
    "    plt.plot(epochs_range, val_accuracies, label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training vs Validation Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcc8a21-52a3-4560-8b33-a12a3841babe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_path = \"DR_DenseNet_Model.pth\"\n",
    "#torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4851e69a-a8ed-40b9-bf06-0d8285cd490a",
   "metadata": {},
   "source": [
    "## Test Set Experiments\n",
    "\n",
    "We want to determine if the enhanced reject quality images have an effect on the accuracy of DenseNet121 model trained on the EyeQ dataset. We begin by defining the test dataset import object that will help load the datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ec8c31-7d10-433e-85ab-bd54a8f6c891",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # for CSV from EyeQ dataset\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# test image loader\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        df = pd.read_csv(csv_file)\n",
    "        \n",
    "        # necessary since enhanced images are PNG and rejects are jpeg\n",
    "        def find_existing_file(image_name):\n",
    "            base_name = os.path.splitext(image_name)[0]\n",
    "            for ext in ['.jpeg', '.png']:\n",
    "                candidate = os.path.join(root_dir, base_name + ext)\n",
    "                if os.path.exists(candidate):\n",
    "                    return candidate\n",
    "            return None\n",
    "\n",
    "        # keep only rows with images in the directory\n",
    "        df['full_path'] = df['image'].apply(find_existing_file)\n",
    "        df = df[df['full_path'].notnull()]\n",
    "\n",
    "        # reset index\n",
    "        self.annotations = df.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # get image path\n",
    "        img_path = self.annotations.iloc[index]['full_path']\n",
    "        # convert image to RGB\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        # add label according to EyeQ dataset\n",
    "        label = self.annotations.iloc[index]['DR_grade']\n",
    "\n",
    "        # transform\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bfab06-bb6b-4649-a282-e778cdecb672",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# transform to imagenet standard\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),   # imagenet size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# load reject images (all quality 0)\n",
    "reject_test_dataset = TestDataset(csv_file='Label_EyeQ_test.csv', root_dir='test/original/Reject', transform=test_transforms)\n",
    "reject_test_loader = DataLoader(reject_test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# load images enhanced with Cofe-Net\n",
    "enhanced_test_dataset = TestDataset(csv_file='Label_EyeQ_test.csv', root_dir='test/enhanced', transform=test_transforms)\n",
    "enhanced_test_loader = DataLoader(enhanced_test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "print(\"Test sets defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f013291-14a3-4f6e-b69e-4ee900a31415",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "reject_labels = reject_test_dataset.annotations['DR_grade']\n",
    "reject_class_counts = reject_labels.value_counts().sort_index()\n",
    "\n",
    "enhanced_labels = enhanced_test_dataset.annotations['DR_grade']\n",
    "enhanced_class_counts = enhanced_labels.value_counts().sort_index()\n",
    "\n",
    "# rejects\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(reject_class_counts.index, reject_class_counts.values, color='skyblue')\n",
    "plt.xlabel('Class Label')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.title('Class Distribution - Reject Fundus Images Test Set')\n",
    "plt.xticks(reject_class_counts.index)\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n",
    "\n",
    "# enhanced\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(enhanced_class_counts.index, enhanced_class_counts.values, color='skyblue')\n",
    "plt.xlabel('Class Label')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.title('Class Distribution - Enhanced Fundus Images Test Set')\n",
    "plt.xticks(enhanced_class_counts.index)\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156c0d1b-e537-4ad1-a657-618eda64c620",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.eval()\n",
    "reject_all_preds = []\n",
    "reject_all_labels = []\n",
    "\n",
    "reject_correct_per_class = torch.zeros(5)\n",
    "reject_total_per_class = torch.zeros(5)\n",
    "\n",
    "with torch.no_grad():\n",
    "    reject_test_iterator = tqdm(reject_test_loader, desc=\"Testing reject images...\", leave=False)\n",
    "    reject_test_correct = 0\n",
    "    reject_test_total = 0\n",
    "    for images, labels in reject_test_iterator:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        \n",
    "        # classwise accuracy\n",
    "        for label, pred in zip(labels, preds):\n",
    "            reject_total_per_class[label] += 1\n",
    "            if label == pred:\n",
    "                reject_correct_per_class[label] += 1\n",
    "        \n",
    "        reject_test_total += labels.size(0)\n",
    "        reject_test_correct += (preds == labels).sum().item()\n",
    "\n",
    "        reject_all_preds.extend(preds.cpu().numpy())\n",
    "        reject_all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "reject_test_accuracy = reject_test_correct / reject_test_total\n",
    "print(f\"Reject Test Acc: {reject_test_accuracy:.4f}\")\n",
    "\n",
    "for i in range(5):\n",
    "    if reject_total_per_class[i] > 0:\n",
    "        acc = 100 * reject_correct_per_class[i] / reject_total_per_class[i]\n",
    "        print(f\"Accuracy for class {i}: {acc:.2f}%\")\n",
    "    else:\n",
    "        print(f\"Class {i} has no samples.\")\n",
    "        \n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "reject_cm = confusion_matrix(reject_all_labels, reject_all_preds)\n",
    "reject_disp = ConfusionMatrixDisplay(confusion_matrix=reject_cm, display_labels=[0, 1, 2, 3, 4])\n",
    "reject_disp.plot(cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3b7031-b4e2-4068-97b7-f8bdc147495d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "enhanced_all_preds = []\n",
    "enhanced_all_labels = []\n",
    "\n",
    "enhanced_correct_per_class = torch.zeros(5)\n",
    "enhanced_total_per_class = torch.zeros(5)\n",
    "\n",
    "with torch.no_grad():\n",
    "    enhanced_test_iterator = tqdm(enhanced_test_loader, desc=\"Testing enhanced images...\", leave=False)\n",
    "    enhanced_test_correct = 0\n",
    "    enhanced_test_total = 0\n",
    "    for images, labels in enhanced_test_iterator:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        \n",
    "        # classwise accuracy\n",
    "        for label, pred in zip(labels, preds):\n",
    "            enhanced_total_per_class[label] += 1\n",
    "            if label == pred:\n",
    "                enhanced_correct_per_class[label] += 1\n",
    "        \n",
    "        enhanced_test_total += labels.size(0)\n",
    "        enhanced_test_correct += (preds == labels).sum().item()\n",
    "\n",
    "        enhanced_all_preds.extend(preds.cpu().numpy())\n",
    "        enhanced_all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "enhanced_test_accuracy = enhanced_test_correct / enhanced_test_total\n",
    "print(f\"Enhanced Test Acc: {enhanced_test_accuracy:.4f}\")\n",
    "\n",
    "for i in range(5):\n",
    "    if enhanced_total_per_class[i] > 0:\n",
    "        acc = 100 * enhanced_correct_per_class[i] / enhanced_total_per_class[i]\n",
    "        print(f\"Accuracy for class {i}: {acc:.2f}%\")\n",
    "    else:\n",
    "        print(f\"Class {i} has no samples.\")\n",
    "\n",
    "enhanced_cm = confusion_matrix(enhanced_all_labels, enhanced_all_preds)\n",
    "enhanced_disp = ConfusionMatrixDisplay(confusion_matrix=enhanced_cm, display_labels=[0, 1, 2, 3, 4])\n",
    "enhanced_disp.plot(cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
